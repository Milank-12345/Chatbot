# import os
# import streamlit as st
# from dotenv import load_dotenv
# from langchain_google_genai import ChatGoogleGenerativeAI
# from langchain.chains import LLMChain
# from langchain.prompts import PromptTemplate
# from langchain.memory import ConversationBufferMemory

# # Load environment variables
# load_dotenv()

# # Page configuration
# st.set_page_config(page_title="üí¨ Loan Advisory Assistant", layout="centered", page_icon="üí∏")

# # App Header
# st.markdown("""
#     <div style='text-align: center; margin-top: -40px;'>
#         <h1 style='color: #4A90E2;'>üí¨ Loan Advisory Assistant</h1>
#         <p style='color: #888;'>Smart, friendly, and tailored loan guidance.</p>
#     </div>
# """, unsafe_allow_html=True)

# # Sidebar Info
# with st.sidebar:
#     st.header("‚ÑπÔ∏è How it works")
#     st.markdown("""
#         This chatbot helps you:
#         - Understand loan types
#         - Choose the best loan
#         - Estimate repayment terms
#         - Get step-by-step guidance

#         Powered by:
#         - üåê **Google Gemini Flash**
#         - ü¶ú **LangChain**
#         - üöÄ **Streamlit**
#     """)
#     if st.button("üßπ Clear Chat"):
#         st.session_state.chat_history = []
#         st.session_state.memory.clear()
#         st.success("Chat history cleared!")

# # API key loading
# api_key = os.getenv("GOOGLE_API_KEY")
# if not api_key:
#     st.error("`GOOGLE_API_KEY` not found in .env file.")
#     st.stop()

# # Initialize LLM
# llm = ChatGoogleGenerativeAI(
#     model="gemini-2.0-flash",
#     google_api_key=api_key,
#     temperature=0.7,
# )

# # Prompt template
# prompt_template = """
# You are a friendly and professional loan advisory assistant. Your goal is to help users find the best loan options by asking relevant questions and providing tailored advice.

# **Important Instructions:**
# - Ask only one question at a time.
# - Use a conversational tone and guide the user step-by-step.
# - Do not suggest loan providers yourself. Recommendations will be generated by the system after gathering all required info.
# - Required fields:1. User's name
# 2. City or location
# 3. Loan type (e.g., personal, home, business, education)
# 4. Loan purpose
# 5. Loan amount
# 6. Credit score
# 7. Monthly income
# 8. Employment type (salaried or self-employed)
# 9. Job/business experience (years)
# 10. Loan tenure (in years)

# If the user's name is not known, ask: "Hi there! May I know your name so I can assist you better?"

# After that, continue asking one relevant question at a time to collect the loan details.

# Current conversation history:
# {chat_history}

# User's latest input: {user_input}

# Ask the next relevant question or give a 20-22 words response based on what the user shared. Remember: only one question at a time.

# Response:
# """

# # Setup session state
# if "memory" not in st.session_state:
#     st.session_state.memory = ConversationBufferMemory(memory_key="chat_history")

# if "chain" not in st.session_state:
#     prompt = PromptTemplate(input_variables=["chat_history", "user_input"], template=prompt_template)
#     st.session_state.chain = LLMChain(llm=llm, prompt=prompt, memory=st.session_state.memory)

# if "chat_history" not in st.session_state:
#     st.session_state.chat_history = []

# # First-time greeting
# if not st.session_state.chat_history:
#     with st.chat_message("assistant", avatar="ü§ñ"):
#         st.markdown("üëã Hello! I'm your personal loan advisor. Let's find the best loan for your needs.")

# # Show chat history
# for user_msg, bot_msg in st.session_state.chat_history:
#     with st.chat_message("user", avatar="üë§"):
#         st.markdown(f"**You:** {user_msg}")
#     with st.chat_message("assistant", avatar="ü§ñ"):
#         st.markdown(f"**Advisor:** {bot_msg}")

# # User input and LLM response
# user_input = st.chat_input("Type your loan-related question...")

# if user_input:
#     with st.chat_message("user", avatar="üë§"):
#         st.markdown(f"**You:** {user_input}")

#     with st.spinner("ü§î Thinking..."):
#         response = st.session_state.chain.run(user_input=user_input)

#     with st.chat_message("assistant", avatar="ü§ñ"):
#         st.markdown(f"**Advisor:** {response}")

#     st.session_state.chat_history.append((user_input, response))

from flask import Flask, render_template, request, jsonify, session
from dotenv import load_dotenv
import os
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory
from langchain.schema import messages_from_dict, messages_to_dict

# Load environment variables
load_dotenv()

app = Flask(__name__)
app.secret_key = os.getenv("FLASK_KEY")  # Required for session
# API key loading
api_key = os.getenv("GOOGLE_API_KEY")
if not api_key:
    raise RuntimeError("`GOOGLE_API_KEY` not found in .env file.")

# Initialize LLM (we'll create this fresh each time)
def create_llm():
    return ChatGoogleGenerativeAI(
        model="gemini-2.0-flash",
        google_api_key=api_key,
        temperature=0.7,
    )

# Prompt template
prompt_template = """
You are a friendly and professional loan advisory assistant. Your goal is to help users find the best loan options by asking relevant questions and providing tailored advice.

**Important Instructions:**
- Ask only one question at a time.
- Use a conversational tone and guide the user step-by-step.
- Do not suggest loan providers yourself. Recommendations will be generated by the system after gathering all required info.
- Required fields:1. User's name
2. City or location
3. Loan type (e.g., personal, home, business, education)
4. Loan purpose
5. Loan amount
6. Credit score
7. Monthly income
8. Employment type (salaried or self-employed)
9. Job/business experience (years)
10. Loan tenure (in years)

If the user's name is not known, ask: "Hi there! May I know your name so I can assist you better?"

After that, continue asking one relevant question at a time to collect the loan details.

Current conversation history:
{chat_history}

User's latest input: {user_input}

Ask the next relevant question or give a 20-22 words response based on what the user shared. Remember: only one question at a time.

Response:
"""

def initialize_session():
    if 'memory' not in session:
        memory = ConversationBufferMemory(memory_key="chat_history")
        session['memory'] = {
            'messages': messages_to_dict(memory.chat_memory.messages),
            'memory_key': memory.memory_key,
            'return_messages': memory.return_messages
        }
    
    if 'chat_history' not in session:
        session['chat_history'] = []
        # First-time greeting
        session['chat_history'].append(("", "üëã Hello! I'm your personal loan advisor. Let's find the best loan for your needs."))

def create_chain():
    # Reconstruct memory from session
    memory = ConversationBufferMemory(
        memory_key=session['memory']['memory_key'],
        return_messages=session['memory']['return_messages']
    )
    memory.chat_memory.messages = messages_from_dict(session['memory']['messages'])
    
    # Create fresh LLM and chain
    llm = create_llm()
    prompt = PromptTemplate(input_variables=["chat_history", "user_input"], template=prompt_template)
    chain = LLMChain(llm=llm, prompt=prompt, memory=memory)
    return chain

@app.route('/', methods=['GET', 'POST'])
def home():
    initialize_session()
    
    if request.method == 'POST':
        if request.form.get('action') == 'clear_chat':
            session['chat_history'] = []
            memory = ConversationBufferMemory(memory_key="chat_history")
            session['memory'] = {
                'messages': messages_to_dict(memory.chat_memory.messages),
                'memory_key': memory.memory_key,
                'return_messages': memory.return_messages
            }
            return jsonify({'status': 'success', 'message': 'Chat history cleared!'})
        
        user_input = request.form.get('user_input')
        if user_input:
            chain = create_chain()
            response = chain.invoke({"user_input": user_input})['text']
            
            # Update chat history
            session['chat_history'].append((user_input, response))
            session['memory'] = {
                'messages': messages_to_dict(chain.memory.chat_memory.messages),
                'memory_key': chain.memory.memory_key,
                'return_messages': chain.memory.return_messages
            }
            
            return jsonify({
                'response': response,
                'chat_history': session['chat_history']
            })
    
    return render_template('index.html', chat_history=session.get('chat_history', []))

if __name__ == '__main__':
    app.run(debug=True, host = "0.0.0.0", port=int(os.getenv('PORT', 5000)))