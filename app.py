import os
import streamlit as st
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory

# Load environment variables
load_dotenv()

# Page configuration
st.set_page_config(page_title="ğŸ’¬ Loan Advisory Assistant", layout="centered", page_icon="ğŸ’¸")

# App Header
st.markdown("""
    <div style='text-align: center; margin-top: -40px;'>
        <h1 style='color: #4A90E2;'>ğŸ’¬ Loan Advisory Assistant</h1>
        <p style='color: #888;'>Smart, friendly, and tailored loan guidance.</p>
    </div>
""", unsafe_allow_html=True)

# Sidebar Info
with st.sidebar:
    st.header("â„¹ï¸ How it works")
    st.markdown("""
        This chatbot helps you:
        - Understand loan types
        - Choose the best loan
        - Estimate repayment terms
        - Get step-by-step guidance

        Powered by:
        - ğŸŒ **Google Gemini Flash**
        - ğŸ¦œ **LangChain**
        - ğŸš€ **Streamlit**
    """)
    if st.button("ğŸ§¹ Clear Chat"):
        st.session_state.chat_history = []
        st.session_state.memory.clear()
        st.success("Chat history cleared!")

# API key loading
api_key = os.getenv("GOOGLE_API_KEY")
if not api_key:
    st.error("âŒ `GOOGLE_API_KEY` not found in .env file.")
    st.stop()

# Initialize LLM
llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    google_api_key=api_key,
    temperature=0.7,
)

# Prompt template
prompt_template = """
You are a friendly and professional loan advisory assistant. Your goal is to help users find the best loan options by asking relevant questions and providing tailored advice.

**Important Instructions:**
- Ask only one question at a time.
- Use a conversational tone and guide the user step-by-step.
- Do not suggest loan providers yourself. Recommendations will be generated by the system after gathering all required info.
- Required fields:1. User's name
2. City or location
3. Loan type (e.g., personal, home, business, education)
4. Loan purpose
5. Loan amount
6. Credit score
7. Monthly income
8. Employment type (salaried or self-employed)
9. Job/business experience (years)
10. Loan tenure (in years)

If the user's name is not known, ask: "Hi there! May I know your name so I can assist you better?"

After that, continue asking one relevant question at a time to collect the loan details.

Current conversation history:
{chat_history}

User's latest input: {user_input}

Ask the next relevant question or give a 20-22 words response based on what the user shared. Remember: only one question at a time.

Response:
"""


# Setup session state
if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(memory_key="chat_history")

if "chain" not in st.session_state:
    prompt = PromptTemplate(input_variables=["chat_history", "user_input"], template=prompt_template)
    st.session_state.chain = LLMChain(llm=llm, prompt=prompt, memory=st.session_state.memory)

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# First-time greeting
if not st.session_state.chat_history:
    with st.chat_message("assistant", avatar="ğŸ¤–"):
        st.markdown("ğŸ‘‹ Hello! I'm your personal loan advisor. Let's find the best loan for your needs.")

# Show chat history
for user_msg, bot_msg in st.session_state.chat_history:
    with st.chat_message("user", avatar="ğŸ‘¤"):
        st.markdown(f"**You:** {user_msg}")
    with st.chat_message("assistant", avatar="ğŸ¤–"):
        st.markdown(f"**Advisor:** {bot_msg}")

# User input and LLM response
user_input = st.chat_input("Type your loan-related question...")

if user_input:
    with st.chat_message("user", avatar="ğŸ‘¤"):
        st.markdown(f"**You:** {user_input}")

    with st.spinner("ğŸ¤” Thinking..."):
        response = st.session_state.chain.run(user_input=user_input)

    with st.chat_message("assistant", avatar="ğŸ¤–"):
        st.markdown(f"**Advisor:** {response}")

    st.session_state.chat_history.append((user_input, response))



